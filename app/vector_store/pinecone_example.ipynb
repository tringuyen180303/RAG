{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3578706fdab4a2eb7d8b5b094b314a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents parquet files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                             values  \\\n",
      "240000  515997  [-0.00531694, 0.06937869, -0.0092854, 0.003286...   \n",
      "240001  515998  [-0.09243751, 0.065432355, -0.06946959, 0.0669...   \n",
      "240002  515999  [-0.021924071, 0.032280188, -0.020190848, 0.07...   \n",
      "240003  516000  [-0.120020054, 0.024080949, 0.10693012, -0.018...   \n",
      "240004  516001  [-0.095293395, -0.048446465, -0.017618902, -0....   \n",
      "\n",
      "                                                 metadata  \n",
      "240000  {'text': ' Why is a \"law of sciences\" importan...  \n",
      "240001  {'text': ' Is it possible to format a BitLocke...  \n",
      "240002  {'text': ' Can formatting a hard drive stress ...  \n",
      "240003  {'text': ' Are the new Samsung Galaxy J7 and J...  \n",
      "240004  {'text': ' I just watched an add for Indonesia...  \n"
     ]
    }
   ],
   "source": [
    "from pinecone_datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('quora_all-MiniLM-L6-bm25')\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "\n",
    "# We don't need sparse_values for this demo either so let's drop those as well\n",
    "dataset.documents.drop(['sparse_values'], axis=1, inplace=True)\n",
    "\n",
    "# To speed things up in this demo, we will use 80K rows of the dataset between rows 240K -> 320K\n",
    "dataset.documents.drop(dataset.documents.index[320_000:], inplace=True)\n",
    "dataset.documents.drop(dataset.documents.index[:240_000], inplace=True)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in dataset: 80000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Pinecone API key\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is missing from environment variables!\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "print(\"Successfully connected to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings have dimension 384\n"
     ]
    }
   ],
   "source": [
    "row1 = dataset.documents.iloc[0:1].to_dict(orient=\"records\")[0]\n",
    "dimension = len(row1['values'])\n",
    "print(f\"These embeddings have dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "index_name = 'gen-qa-openai-fast'\n",
    "\n",
    "# Check if index already exists (it shouldn't if this is first time running this demo)\n",
    "if not pc.has_index(name=index_name):\n",
    "    # If does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension, # dimensionality of text-embedding-ada-002\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Instantiate an index client\n",
    "index = pc.Index(name=index_name)\n",
    "\n",
    "# View index stats of our new, empty index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting records batch: 100%|██████████| 800/800 [04:26<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for start in tqdm(range(0, len(dataset.documents), batch_size), \"Upserting records batch\"):\n",
    "    batch = dataset.documents.iloc[start:start + batch_size].to_dict(orient=\"records\")\n",
    "    index.upsert(vectors=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_questions(question):\n",
    "    # Embed the question into a query vector\n",
    "    xq = model.encode(question).tolist()\n",
    "\n",
    "    # Now query Pinecone to find similar questions\n",
    "    return index.query(vector=xq, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '522021',\n",
       "              'metadata': {'text': ' How can I combine those sentences as a '\n",
       "                                   'sentence?'},\n",
       "              'score': 0.543346882,\n",
       "              'values': []},\n",
       "             {'id': '536966',\n",
       "              'metadata': {'text': ' What are the best tools to use for '\n",
       "                                   'Natural Language Processing currently?'},\n",
       "              'score': 0.483892381,\n",
       "              'values': []},\n",
       "             {'id': '61616',\n",
       "              'metadata': {'text': ' I know the basics of English and I can '\n",
       "                                   \"speak it normally, but I don't know about \"\n",
       "                                   'sentence structure. What can help me to '\n",
       "                                   'know sentence structure?'},\n",
       "              'score': 0.47395274,\n",
       "              'values': []},\n",
       "             {'id': '101891',\n",
       "              'metadata': {'text': ' How do I learn Natural Language '\n",
       "                                   'Processing?'},\n",
       "              'score': 0.454279035,\n",
       "              'values': []},\n",
       "             {'id': '73064',\n",
       "              'metadata': {'text': ' What is the best way to learn phrasal '\n",
       "                                   'verbs?'},\n",
       "              'score': 0.453713089,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\n",
    "    \"Which training method should I use for sentence transformers when \" +\n",
    "    \"I only have pairs of related sentences?\"\n",
    ")\n",
    "\n",
    "xq = find_similar_questions(query)\n",
    "xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54:  How can I combine those sentences as a sentence?\n",
      "0.48:  What are the best tools to use for Natural Language Processing currently?\n",
      "0.47:  I know the basics of English and I can speak it normally, but I don't know about sentence structure. What can help me to know sentence structure?\n",
      "0.45:  How do I learn Natural Language Processing?\n",
      "0.45:  What is the best way to learn phrasal verbs?\n"
     ]
    }
   ],
   "source": [
    "def print_query_results(results):\n",
    "    for result in results['matches']:\n",
    "        print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n",
    "\n",
    "print_query_results(xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(OPENAI_API_KEY)\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    \"\"\"Generate response using OpenAI's GPT with retrieved context.\"\"\"\n",
    "    context = \"\\n\".join([doc[\"metadata\"][\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the following context to answer the query.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Query:\n",
    "    {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    temperature=0.5,\n",
    ")\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you only have pairs of related sentences and want to train sentence transformers, one effective method is to use a supervised learning approach with contrastive loss functions, such as the Siamese network architecture. This involves using a dataset of sentence pairs where each pair is labeled with a similarity score or a binary label indicating whether they are related. You can then train the model to minimize the distance between embeddings of related sentence pairs while maximizing the distance between unrelated pairs. Popular frameworks like Hugging Face's Transformers library provide tools and pre-trained models that can be fine-tuned for this purpose using your dataset.\n"
     ]
    }
   ],
   "source": [
    "answer = generate_answer(query, xq['matches'])\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
